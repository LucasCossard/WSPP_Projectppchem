{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsL6ZKH9JH9vAaJCzvPRsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nohalyan/Projetppchem/blob/Lucas1/Notebook_WSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Water Solubility Predisction\n",
        "\n"
      ],
      "metadata": {
        "id": "9we0xsQKCIis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Import Relevant Modules and Libraries\n",
        "\n",
        "Let's first start by importing relevant modules and libraries needed for this project.\n"
      ],
      "metadata": {
        "id": "9VBM8XdnDHfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all libraries\n",
        "!pip install pathlib numpy pandas rdkit matplotlib scikit-learn lightgbm lazypredict tqdm"
      ],
      "metadata": {
        "id": "GFhpnlDlEGpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDEhgWFsCC0T"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "import seaborn as sn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# lazypredict helps to train 42 ML models with a single line of code ans find the best ML models\n",
        "import lazypredict\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Let's get the Solubility Data\n",
        "\n",
        "First, we will get solubility data from gashawmg (source: https://github.com/gashawmg) and perform exploratory data analysis"
      ],
      "metadata": {
        "id": "_HeTUCurHCnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data_Solubility.csv"
      ],
      "metadata": {
        "id": "XiN0eigVHNgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's open the file"
      ],
      "metadata": {
        "id": "MNh8CNiSb3c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Path object for the current directory, in our case /content/\n",
        "current_directory = Path.cwd()\n",
        "print(\"Current Directory:\", current_directory.resolve())\n",
        "\n",
        "file_path = current_directory / \"Data_Solubility.csv\"\n",
        "\n",
        "# Reading the contents of the file and check that the file exists\n",
        "if file_path.exists():\n",
        "    with file_path.open(\"r\") as file:\n",
        "        content = file.read()\n",
        "#       print(content)\n",
        "else:\n",
        "    print(\"The file does not exist.\")\n"
      ],
      "metadata": {
        "id": "NYPnESo9bPPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file use semicicolon as delimiter, so let's open the file and use semicicolon as delimiter:"
      ],
      "metadata": {
        "id": "HhmAA3fHh1kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open a file containing descriptors and yield\n",
        "data_solubility = pd.read_csv(\"/content/Data_Solubility.csv\", delimiter=';')"
      ],
      "metadata": {
        "id": "3cJRK4pwWNwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the data see if it is what we want"
      ],
      "metadata": {
        "id": "gEJ2RcUqhktQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_solubility.shape"
      ],
      "metadata": {
        "id": "2egFBYgjf17S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_solubility.head()"
      ],
      "metadata": {
        "id": "V9zwa1_mhblV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks nice to me!"
      ],
      "metadata": {
        "id": "OFo14mE9hvdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 Data Cleaning\n",
        "## 2.2.1 Remove NaN or null values\n",
        "\n",
        "We wil start by removing non-numerical values and valeurs that are null:"
      ],
      "metadata": {
        "id": "rsPHJbgfinOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_solubility.SMILES.isnull().sum()\n",
        "data_solubility.dropna(inplace=True)\n",
        "data_solubility.shape"
      ],
      "metadata": {
        "id": "6wnyNXZkicm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the shape is still the same, the data has already been cleaned of non-numerical and null values.\n",
        "##2.2.2 Remove outliers\n",
        "\n",
        "Then, we will remove outliers from the data. Using a boxplot, we can easely visualize outliers:\n"
      ],
      "metadata": {
        "id": "QC27TZqri8CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sn.set_theme()\n",
        "sn.displot(data=data_solubility, x=\"logS\", binwidth=1)"
      ],
      "metadata": {
        "id": "kuD5t60TnrX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's filter compounds that follow as close as normal distribution, let's say between -7.5 and 1.7:\n"
      ],
      "metadata": {
        "id": "MSFjaKMyqCV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_solubility = data_solubility[data_solubility.logS.apply(lambda x: x > -7.5 and x < 1.7)]"
      ],
      "metadata": {
        "id": "Ht-zgHIvqIJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate an histogram to see the new data:"
      ],
      "metadata": {
        "id": "QHgDSAQ8rWZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sn.displot(data=new_data_solubility, x='logS', binwidth=1,kde=True)\n",
        "new_data_solubility.shape"
      ],
      "metadata": {
        "id": "9wjGJ5AGrW6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2.3 Remove Duplicates\n",
        "\n",
        "Then remove duplicate by generating canonical SMILES:"
      ],
      "metadata": {
        "id": "lDjkwQEwlcMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a canonical SMILES function\n",
        "def canonical_SMILES(smiles):\n",
        "    canon_smls = [Chem.CanonSmiles(smls) for smls in smiles]\n",
        "    return canon_smls"
      ],
      "metadata": {
        "id": "KydIsfXejaJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate canonical Smiles using the function\n",
        "canon_smiles = canonical_SMILES(new_data_solubility.SMILES)\n",
        "\n",
        "# Replace SMILES column with canonical SMILES\n",
        "new_data_solubility[\"SMILES\"] = canon_smiles\n",
        "\n",
        "# Create a list for duplicate smiles\n",
        "duplicate_smiles = new_data_solubility[new_data_solubility['SMILES'].duplicated()]['SMILES'].values\n",
        "len(duplicate_smiles)"
      ],
      "metadata": {
        "id": "4MDLZcMzjWtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, their are 6 duplicates, so we have to filter them and we can also sort them for better reading:"
      ],
      "metadata": {
        "id": "I7qNi08VkYMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_solubility[new_data_solubility['SMILES'].isin(duplicate_smiles)].sort_values(by=['SMILES'])"
      ],
      "metadata": {
        "id": "P73Yy1K7kXzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's drop rows that contain duplicate SMILES and keep the first structure:"
      ],
      "metadata": {
        "id": "QRSA4J581bCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_solubility_cleaned = new_data_solubility.drop_duplicates(subset=['SMILES'], keep='first')\n",
        "data_solubility_cleaned.shape\n",
        "data_solubility_cleaned.head()"
      ],
      "metadata": {
        "id": "jexwvrgv1lSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.4 Filter training data\n",
        "\n",
        "Now that we have a dataset, let's prepapare a test stet containing 100 drug-like compounds (source: https://github.com/PatWalters/solubility)"
      ],
      "metadata": {
        "id": "HXSd0vY02KUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data_Drug_Like_Solubility.csv"
      ],
      "metadata": {
        "id": "6AsBNHVfErm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Path object for the current directory, in our case /content/\n",
        "current_directory_dl = Path.cwd()\n",
        "print(\"Current Directory:\", current_directory.resolve())\n",
        "\n",
        "file_path_dl = current_directory / \"Data_Drug_Like_Solubility.csv\"\n",
        "\n",
        "# Reading the contents of the file and check that the file exists\n",
        "if file_path.exists():\n",
        "    with file_path.open(\"r\") as file:\n",
        "        content = file.read()\n",
        "#        print(content)\n",
        "else:\n",
        "    print(\"The file does not exist.\")\n"
      ],
      "metadata": {
        "id": "fxxXu8QdElDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dl = pd.read_csv(\"/content/Data_Drug_Like_Solubility.csv\", delimiter=';')\n",
        "data_dl.shape"
      ],
      "metadata": {
        "id": "9uz_TeGiFRWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dl.head()"
      ],
      "metadata": {
        "id": "2U7jAFxBGt4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate canonical Smiles\n",
        "canon_smiles = canonical_SMILES(data_dl.SMILES)\n",
        "\n",
        "# Replace SMILES column wit Canonical SMILES\n",
        "data_dl[\"SMILES\"] = canon_smiles\n",
        "\n",
        "# Create a list for duplicate smiles\n",
        "duplicate_data_dl_smiles = data_dl[data_dl['SMILES'].duplicated()]['SMILES'].values\n",
        "len(duplicate_data_dl_smiles)"
      ],
      "metadata": {
        "id": "tLHQ1dgrHL-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Molecules used in training and test of the model\n",
        "data_dl_SMILES = data_dl.SMILES.values\n",
        "\n",
        "# Filter molecules that are not present in the test set\n",
        "data_cleaned_final = data_solubility_cleaned[~data_solubility_cleaned['SMILES'].isin(data_dl_SMILES)]\n",
        "print(f'Compounds present in training set:{len(data_solubility_cleaned) - len(data_cleaned_final)}')\n",
        "data_cleaned_final.shape"
      ],
      "metadata": {
        "id": "akgVYCbtHqwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dl= data_dl[data_dl['LogS exp (mol/L)'].apply(lambda x: x > -7.5 and x < 1.7)]\n",
        "data_dl"
      ],
      "metadata": {
        "id": "jEoNO8CMLVqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Calculation of RDkit Molecular Descriptors, which are molecular features"
      ],
      "metadata": {
        "id": "IwdeeEJMLtxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RDkit_descriptors(smiles):\n",
        "    mols = [Chem.MolFromSmiles(i) for i in smiles]\n",
        "    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0]\n",
        "                                    for x in Descriptors._descList])\n",
        "    desc_names = calc.GetDescriptorNames()\n",
        "\n",
        "    Mol_descriptors =[]\n",
        "    for mol in tqdm(mols):\n",
        "        # add hydrogens to molecules\n",
        "        mol=Chem.AddHs(mol)\n",
        "        # Calculate all 200 descriptors for each molecule\n",
        "        descriptors = calc.CalcDescriptors(mol)\n",
        "        Mol_descriptors.append(descriptors)\n",
        "    return Mol_descriptors,desc_names\n",
        "\n",
        "# Function call\n",
        "Mol_descriptors,desc_names = RDkit_descriptors(data_cleaned_final['SMILES'])"
      ],
      "metadata": {
        "id": "kiwjxW8iL39L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_descriptors = pd.DataFrame(Mol_descriptors, columns=desc_names)\n",
        "df_descriptors.head()"
      ],
      "metadata": {
        "id": "1v7UGPE6OLKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_descriptors.shape"
      ],
      "metadata": {
        "id": "PLerraWYQhCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Split the chemicals for training and validation set"
      ],
      "metadata": {
        "id": "KZFGKSecQqsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(df_descriptors, data_cleaned_final.logS, test_size=0.1,random_state=42)\n",
        "\n",
        "#Standardization of the features\n",
        "\n",
        "custom_scaler = StandardScaler()\n",
        "custom_scaler.fit(x_train)\n",
        "x_train_scaled = custom_scaler.transform(x_train)\n",
        "x_valid_scaled = custom_scaler.transform(x_valid)"
      ],
      "metadata": {
        "id": "xtfxtv4bQ1JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Select Machine Learning Models\n",
        "\n",
        "Let's selct the best ML models for that. To do that, we will use the lazypredict librarie, in particular the LazyRegressor function to test 42 ML models:"
      ],
      "metadata": {
        "id": "ObN4zyriRhtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lregs = LazyRegressor(verbose=0,ignore_warnings=True, custom_metric=None,random_state=42)\n",
        "models, prediction_tests = lregs.fit(x_train_scaled, x_valid_scaled, y_train, y_valid)"
      ],
      "metadata": {
        "id": "fVj91hWoRkr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The top three models\n",
        "prediction_tests[:5]"
      ],
      "metadata": {
        "id": "djOzUCFpWF-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Fine-tuning of LGBMRegressor\n",
        "\n",
        "We decided to take the LGBMRegressor model because the results generated by this model are comparable to the ExtraTreesRegressor model, but takes a lot less time than the extra-trees model.Let's performs a grid search using GridSearchCV from scikit-learn to find the best hyperparameters for a LightGBM regressor:"
      ],
      "metadata": {
        "id": "WHz1RVk6WlW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#params = {'max_depth' : list(range(2, 32, 8)),\n",
        "#          'n_estimators' : list(range(1, 1000, 100)),\n",
        "#          'learning_rate' : list(np.arange(0.01, 1.02, 0.25))}\n",
        "#\n",
        "#grid_search = GridSearchCV(LGBMRegressor(random_state = 42),\n",
        "#                            param_grid=params, cv=5, verbose=1)\n",
        "#\n",
        "#grid_search.fit(x_train, y_train)\n",
        "#\n",
        "#print(\"Optimized parameters for a LightGBM regressor can be: \", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "4C5cOJ4hXG3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtained:\n",
        "* learning_rate: 0.01\n",
        "* max_depth: 26\n",
        "* n_estmitors: 901\n",
        "\n",
        "Let's optimize again with new ranges for liste max_depth, n_estmitors and learning_rate to obtain even better parameters:\n"
      ],
      "metadata": {
        "id": "ELET-iZOcmPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#params_bst = {\"max_depth\" : list(range(20, 36, 4)),\n",
        "#         \"n_estimators\" : list(range(850, 1200, 50)),\n",
        "#         \"learning_rate\" : list(np.arange(0.01, 0.05, 0.01))}\n",
        "#\n",
        "#grid_search_bst = GridSearchCV(LGBMRegressor(random_state = 42),\n",
        "#                                  param_grid=params_bst, cv=3, verbose=1)\n",
        "#\n",
        "#grid_search_bst.fit(x_train, y_train)\n",
        "#print(\"The best parameters are: \", grid_search_bst.best_params_)"
      ],
      "metadata": {
        "id": "Kwe8dQKHNTzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. LGBMRegressor model for training and test data\n",
        "Let's test our model ont the training and test set with the best parameters found with the fine tunning:\n",
        "* learning_rate: 0.04\n",
        "* max_depth: 24\n",
        "* n_estmitors: 1150\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dfuMUiEYAu01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LGBMRegressor(n_estimators = 1150,\n",
        "                      max_depth = 24,\n",
        "                      learning_rate = 0.04,\n",
        "                      random_state= 42)\n",
        "\n",
        "model.fit(x_train_scaled,y_train)\n",
        "y_preds = model.predict(x_valid_scaled)"
      ],
      "metadata": {
        "id": "lV5YnDz4BOhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A plotting function\n",
        "def plot_data(actual, predicted, title):\n",
        "\n",
        "# model performance using RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "\n",
        "# R^2 (coefficient of determination) :\n",
        "    R2 =r2_score(actual, predicted)\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "# Plot the figure\n",
        "    sn.regplot(x=predicted , y=actual,line_kws={\"lw\":2,\n",
        "                                                \"ls\":\"--\",\n",
        "                                                \"color\":\"red\",\n",
        "                                                \"alpha\":0.7})\n",
        "    plt.title(title, color=\"red\")\n",
        "    plt.xlabel(\"Predicted logS(mol/L)\",\n",
        "               color=\"blue\")\n",
        "    plt.xlim(-8,1)\n",
        "    plt.ylabel(\"Experimental logS(mol/L)\",\n",
        "               color =\"blue\")\n",
        "\n",
        "\n",
        "    plt.grid(alpha=0.3)\n",
        "    R2 = mpatches.Patch(label=\"R2={:04.2f}\".format(R2))\n",
        "    rmse = mpatches.Patch(label=\"RMSE={:04.2f}\".format(rmse))\n",
        "    plt.legend(handles=[R2, rmse])"
      ],
      "metadata": {
        "id": "vYAhA4_1ChMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the predicted logS of the validation set and see if our model works:"
      ],
      "metadata": {
        "id": "oLXafM4bEpJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sn.set_theme(style=\"whitegrid\")\n",
        "plot_data(y_valid,y_preds,\"Validation data\")"
      ],
      "metadata": {
        "id": "xtO2vsUUEnMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate molecular descriptors for the test data or 98 compounds\n",
        "\n",
        "Mol_descriptors_test , desc_names_test = RDkit_descriptors(data_dl[\"SMILES\"])\n",
        "data_dl_descriptors = pd.DataFrame(Mol_descriptors_test,columns=desc_names_test)\n",
        "\n",
        "# Standard scaler - transform\n",
        "x_scaled_test = custom_scaler.transform(data_dl_descriptors)\n",
        "\n",
        "# Predict solubility of the test data\n",
        "y_test_preds = model.predict(x_scaled_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "PzzUsNv0En7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting testing set\n",
        "sn.set_theme(style=\"whitegrid\")\n",
        "plot_data(data_dl[\"LogS exp (mol/L)\"], y_test_preds,\n",
        "           \"Test data: Drug-like Molecules\")"
      ],
      "metadata": {
        "id": "hf_U34X3eEju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Saving of the trained model and standard scaler"
      ],
      "metadata": {
        "id": "dG-nNTVGJGQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"model_WSP.pkl\",\"wb\") as f:\n",
        "    pickle.dump(model,f)\n",
        "\n",
        "with open(\"scaler_WSP.pkl\",\"wb\") as f:\n",
        "    pickle.dump(custom_scaler,f)"
      ],
      "metadata": {
        "id": "LBUk2hL7JdwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}